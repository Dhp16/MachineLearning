{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is notebook 2, and your attention is required! This is DEEP learning because the surface ain't enough!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning involves using more than one hidden layer, aka all code from previous book plus some extra parametrization to get a larger number of layers in between. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data as yesterday for the moment\n",
    "# import Box\n",
    "\n",
    "# Import box\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plot\n",
    "import copy \n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "breastCancerDataset = load_breast_cancer()\n",
    "\n",
    "m_total = len(breastCancerDataset.target)\n",
    "y = breastCancerDataset.target.reshape(m_total, 1)\n",
    "x = breastCancerDataset.data[:, np.r_[0:10]]\n",
    "x = preprocessing.scale(x)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "xTrain = xTrain.T\n",
    "xTest = xTest.T\n",
    "yTrain = yTrain.T\n",
    "yTest = yTest.T\n",
    "\n",
    "nFeatures = xTrain.shape[0]\n",
    "nNodes = nFeatures+2\n",
    "\n",
    "if(debug):\n",
    "    print(\"xTrain: \", xTrain.shape)\n",
    "    print(\"nFeatures: \", nFeatures)\n",
    "    print(\"nNodes: \", nNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define those activation functions\n",
    "\n",
    "def leakyReLU(Z, slope=0.05):            # Rectified Linear Unit function\n",
    "    return np.maximum(slope*Z, Z)\n",
    "\n",
    "def deltaLeakyReLU(Z, slope=0.05):\n",
    "    Z[Z>0] = 1\n",
    "    Z[Z<0] = slope  \n",
    "    return Z\n",
    "    \n",
    "def sigmoid(Z, factor=1):                # pretty much tanh(x) but not really\n",
    "    return 1/(1+np.exp(-Z*factor))\n",
    "\n",
    "def deltaSigmoid(Z, factor=1):\n",
    "    return np.multiply(Z*factor, (1-Z)*factor)\n",
    "\n",
    "def activation(Z, function):\n",
    "    if(function == \"leakyReLU\"):\n",
    "        K = leakyReLU(Z)\n",
    "        return K\n",
    "    else:\n",
    "        return sigmoid(Z)\n",
    "    \n",
    "def costCompute(Y, A):               # cost function to be found in the coursera lectures\n",
    "    sample = np.multiply(Y, np.log(A)) + np.multiply(1-Y, np.log(1.000001-A))\n",
    "    sample = np.sum(sample)\n",
    "    return -sample/Y.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseParameters(networkParameters, X):\n",
    "    Ws = []\n",
    "    Bs = []\n",
    "    nHiddenLayers = networkParameters[0]\n",
    "    layerNodes = networkParameters[1]\n",
    "    layerNodes.append(1)\n",
    "    layerNodes.insert(0, X.shape[0])        #for the first hidden layer, the number of 'previous nodes' is the number of feature\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(1, nHiddenLayers+2):\n",
    "        W = np.random.rand(layerNodes[i], layerNodes[i-1])*0.01\n",
    "        B = np.random.rand(layerNodes[i], 1)                         # conseil de Ambroise de ne pas mettre m (end 2eme dim)          \n",
    "        previousNodes = W.shape[1]\n",
    "        Ws.append(W)\n",
    "        Bs.append(B)\n",
    "\n",
    "    return (Ws, Bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateActivationList(nHiddenLayers):\n",
    "    activationList = []\n",
    "    for i in range(0, nHiddenLayers):\n",
    "        activationList.append(\"leakyReLU\")\n",
    "    activationList.append(sigmoid)\n",
    "    return activationList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardProcess(A, W, b):   # .T --> transpose\n",
    "    return np.dot(W, A)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(coefficientsSet, Ws, bs, learningRate=1.0):\n",
    "    for i in range(nHiddenLayers, -1, -1):\n",
    "        Ws[i] = Ws[i] - coefficientsSet[i][\"dW\"]*learningRate\n",
    "        bs[i] = bs[i] - coefficientsSet[i][\"db\"]*learningRate\n",
    "    return (Ws, bs)\n",
    "\n",
    "def backwardPropagation(Y, Ws, bs, As, Zs, nHiddenLayers, deltaActivationList, learningRate):\n",
    "    m = Y.shape[1]\n",
    "   \n",
    "    dZnow = As[-1] - Y                        # A[4]\n",
    "    \n",
    "    coefficientsSet = []\n",
    "        \n",
    "    # need to use delta Sigmoid    \n",
    "        \n",
    "    for i in range(nHiddenLayers, -1, -1):                 # counting downwards\n",
    "        if(i != nHiddenLayers):\n",
    "            dZnow = np.multiply(np.dot(Ws[i+1].T, dZnow), deltaLeakyReLU(Zs[i]))\n",
    "\n",
    "        dW = 1/m*np.dot(dZnow, As[i].T)                    # not sure about the A index\n",
    "        db = 1/m*np.sum(dZnow, axis=1, keepdims=True)\n",
    "                                    \n",
    "        coefficients = {\n",
    "            \"dW\" : dW,\n",
    "            \"db\" : db\n",
    "        }\n",
    "        coefficientsSet.append(coefficients)\n",
    "    \n",
    "    coefficientsSet.reverse()\n",
    "    Ws, bs = updateParameters(coefficientsSet, Ws, bs, learningRate)\n",
    "\n",
    "    return (Ws, bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepModel(X, Y, nLayers, networkParameters, nFeatures, nIterations=5, debug=False, \n",
    "          learningRate=0.01):\n",
    "      \n",
    "    Ws, bs = initialiseParameters(networkParameters, X)\n",
    "    activationList = generateActivationList(networkParameters[0])\n",
    "    deltaActivationList = [\"deltaLeakyRelU\",\"deltaLeakyReLU\",\"deltaLeakyReLU\",\"deltaSigmoid\"]\n",
    "    \n",
    "    Cost = []\n",
    "    print(\"Y shape: \", Y.shape)\n",
    "    \n",
    "    for i in range(0, nIterations):\n",
    "        if(debug):\n",
    "            print('\\n\\n\\nIteration ', i)\n",
    "        A = copy.deepcopy(X)\n",
    "        As = []\n",
    "        As.append(A)\n",
    "        Zs = []\n",
    "        for j in range(0, nHiddenLayers+1):\n",
    "            if(debug):\n",
    "                print(\"\\n   -----------------  layer: \", j, \"   ---------------------   \")\n",
    "            Z = feedForwardProcess(A, Ws[j], bs[j])\n",
    "            A = activation(Z, activationList[j])\n",
    "            if(debug):\n",
    "                print(\"A size: \", A.shape, \" Z size: \", Z.size)\n",
    "            As.append(A)\n",
    "            Zs.append(Z)\n",
    "        cost = costCompute(Y, A)\n",
    "        Cost.append(cost)\n",
    "        Ws, bs = backwardPropagation(Y, Ws, bs, As, Zs, nHiddenLayers, deltaActivationList, learningRate)\n",
    "    print(\"Final Cost: \", cost)\n",
    "    \n",
    "    \n",
    "    plot.figure(1)\n",
    "    plot.plot(Cost)              #{-x for x in c}\n",
    "    plot.ylabel('The Cost Function ($)')\n",
    "    plot.xlabel('nIterations')\n",
    "    plot.show()\n",
    "    \n",
    "    return Ws, bs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(xTest, Ws, bs, nHiddenLayers):\n",
    "    activationList = generateActivationList(nHiddenLayers)\n",
    "    A = copy.deepcopy(xTest)\n",
    "    for j in range(0, nHiddenLayers+1):\n",
    "        Z = feedForwardProcess(A, Ws[j], bs[j])\n",
    "        A = activation(Z, activationList[j])\n",
    "    prediction = np.round(A)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layerNodes = [10, 10, 10]\n",
    "nHiddenLayers = len(layerNodes)\n",
    "networkParameters = [nHiddenLayers, layerNodes]\n",
    "\n",
    "Ws, bs = deepModel(xTrain, yTrain, 5, networkParameters, nFeatures, 10000, False, 0.5)\n",
    "\n",
    "predictions = prediction(xTest, Ws, bs, nHiddenLayers)\n",
    "\n",
    "print(\"Predictions with test set: \")\n",
    "print ('Accuracy: %d' % float((np.dot(yTest, predictions.T)+ np.dot(1 - yTest, 1 - predictions.T))\n",
    "                              / float(yTest.size) * 100) + '%')\n",
    "\n",
    "\n",
    "predictions2 = prediction(xTrain, Ws, bs, nHiddenLayers)\n",
    "print(\"Predictions with training set: \")\n",
    "print ('Accuracy: %d' % float((np.dot(yTrain, predictions2.T)+ np.dot(1 - yTrain, 1 - predictions2.T))\n",
    "                              / float(yTrain.size) * 100) + '%')\n",
    "\n",
    "\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
